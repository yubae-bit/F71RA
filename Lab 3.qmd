---
title: "Lab 3"
author: "Yubo Rasmussen"
format: docx
editor: visual
---

## Lab 3

Today we are going to explore "Factors in R" and "Linear Regression".

## Factors

First load the following libraries:

```{r}
library(forcats)   
library(ggplot2)
library(tidyverse)
```

### Sorting a vector

Here is a vector with the names of some months, which we want to sort, but can't use the sort function.

```{r}
# Vector with the names of some months:
x1 <- c("Dec", "Apr", "Jan", "Mar")
# We want to sort this vector but sort()
# will not produce the desired outcome in this case
sort(x1)
```

Instead, we need to do this:

```{r}

```

### Star Wars Example

Star Wars Data: A summary of the frequency of each species in the star wars universe.

First attach the star wars dataframe from `dplyr`

```{r}
```

Suppose that we want to a summary of the frequency of each species in the star wars universe.

```{r}
```

We observe that several species have very low frequency so it would make sense to lump them together using the `fct_lump()` function and create a new group called "other" which, for example, will include all the species with frequency less than 3.

```{r}
```

Suppose now, that we want to visualize the frequency of eye colors of all the characters in the star wars universe.

```{r}
```

Also, we can order the bars in the previous chart according to their frequency using the `fct_infreq()` function:

```{r}

```

Finally, we may want to visualize the summary statistics of a certain variable based on a specific categorical feature.

For example, it is interesting to see how average mass of the species in the star wars universe varies across different eye colors.

```{r}
```

Finally, we can reorder factor levels by sorting along another variable using the `fct_reorder()` function. As an example we will reorder the levels of the variable eye color accoridng to the variable average mass:

```{r}
```

## Linear Regression

Linear Regression is a supervised machine learning algorithm which is used to predict the value of an outcome variable, or response, $Y$, based on one or more input features, or explanatory, variables $X$.

Suppose that $Y_i$ is the $i$th observation of the dependent variable and $X_{ij}$ is $i$th observation of the $j$th independent variable, for $i = 1, ..., n$ and $j = 1, 2, ..., p.$ Then, the basic model for multiple linear regression is $X$ and the response variable $Y$.

$$Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_px_{pi}+\varepsilon_i$$ where $\varepsilon_i \sim N(0,\sigma^2)$ is an i.i.d. random variable.

A multiple linear regression can be fitted with the `lm()` function in R. In our numerical analysis, we will demonstrate linear regression in a simple and easy to understand fashion by using the built-in data set "`cars`" that comes with R by default.\

Response variable $Y$:

-   `mpg` (miles/gallon)\

Explanatory Variables $X$:

-   `cyl` (Number of cylinders),

-   `hp` (gross horsepower) and

-   `wt` (weight).

```{r}
```

We will investigate the relationship between the response mpg and features: `cyl`, `hp` and `wt`.

```{r}

```

The estimated regression coefficients $\beta_i$, $i = 1,...,3$ are shown in the 'Estimate' column.

The fitted linear model is:

$$ \hat{\texttt{mpg}} = 38.75179 - 0.94162 \texttt{cyl} - 0.01904 \texttt{hp} - 3.16697 \texttt{wt}$$Look at the significance codes. What do they tell us?

Copy and paste the results into ChatGPT and ask it to interpret it for us. Do you agree with what it says?

**"Heavier cars with more cylinders and higher horsepower generally use more fuel. But in this model, weight is the key factor driving mpg differences."**

### Residuals

A good way to test the quality of the fit of the model is via a graphical investigation of the residuals $\varepsilon_i$, which are the differences between the real values and the predicted values $\varepsilon_i = y_i - \hat{y_i}$.

To access the residuals of a linear model, use `residuals()` function:

```{r}

```

The pattern of residuals shown in plot should exhibit a random pattern (fluctuate around 0).

Quantile-Quantile (QQ) plots) let you check that the data meet the assumption of normality. In particular, Q-Q plots take your sample data, sort it in ascending order, and then plot them versus the quantiles calculated from a theoretical distribution, in this case the normal distribution. If the residuals are from a normal population, they should fall close to the straight line.

### Goodness-of-fit

We can also look at $R^2$ and $\bar{R^2}$ (adjusted $R^2$) from `summary(fitted LM).` Their formulas are:

$$
R^2 = 1 - \frac{\sum_i(y_i - \hat{y_i})^2}{\sum_i(y_i - \bar{y_i})^2} \text{   and   } \bar{R}^2 = 1 - (1-R^2)\frac{n-1}{n-p-1}
$$

-   $R^2$ is defined as the proportion of the total variability explained by the regression model.

-   One problem with this $R^2$ is that it invariably increases as you add more explanatory variables $x$ to your model. This encourages adding extra useless features.

-   The adjusted $\bar{R}^2$ helps to mitigate this issue by penalizing additional variables.

## Case Study: A Motor Third-party Liability Policies

Load the data from the previous lab our data-set on MTPL claim counts.

```{r}
```

1.  Fit a linear regression model with `ClaimNb` as the response variable.

```{r}
```

2.  Perform a residual analysis of our fitted model. You would expect things to look strange.

```{r}


```

3.  Now let us assume that claim counts follow a Poisson distribution. This will allow us to account for exposure. Fit a GLM using `gamlss`. Use only 10% of the data as otherwise running the regression model over the entire dataset will take too long.

```{r}

```

4.  Perform residual analysis on this. Which model do you think is better, the Poisson or Linear Regression one?

```{r}


```
