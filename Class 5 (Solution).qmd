---
title: "Class 5 (Solution)"
format: html
editor: visual
toc: true
---

## Class 5 (23/10/2023)

This week, we will be going through the codes from week 5 and 7 lectures. That is, continue learning about PCA. The codes throughout this notebook comes from "R Code Lecture Slides Part III; IV A and IV B".

## PCA

Recall that we use PCA to reduce the dimension for continuous explanatory variables $x$ using principal components and complicated mathematics. We have seen in the class last week that there are many packages that runs PCA for us. But how can we trust that the packages work properly? I would say that R its CRAN repositories is pretty trustworthy, it is managed by a large multinational corporation Posit, which is used in many many companies in the world. However, in actuarial work, we need to explain how we got to the specific answer in as much detail as possible because the regulators tell us to... Therefore, we need to try and understand how the PCs are computed mathematically. It is also important that our work can be reproduced using a different dataset to `mtcars`, so let us perform PCA again on the `SportsCars` dataset. 

Remove all objects from the R memory. What is the command for this? 

```{r}
rm(list=ls())
```


### R Code Lecture Slides Part III

Load the tidyverse, ggplot and GGally libraries

```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
```

Go onto canvas and copy and paste the codes in the chunck below:

```{r}
# Read the SportsCars.csv and name this SC
SC <- read.csv('SportsCars.csv', sep = ';', header = TRUE)

# Display the structure of this data frame
str(SC)

# Head and tails
head(SC)
tail(SC)

#Summary
summary(SC)

# Add the log-transformation
SC <- SC %>% mutate(W_log = log(weight),
MP_log <- log(max_power),
CC_log <- log(cubic_capacity),
MT_log <- log(max_torque),
MES_log <- log(max_engine_speed),
S100_log <- log(seconds_to_100),
TS_log <- log(top_speed))

S100_log <- log(SC$seconds_to_100)

# Use ggpairs() to produce a scatterplot
ggpairs(SC %>% filter(!is.na(S100_log)), columns = 14:20)

# Classify the cars into the groups of tau
SC <- SC %>% mutate(sports_type = cut(tau, breaks = c(0, 17, 21, 100), 
                                   labels = c('tau<17 (sports car)', 
                                              '17<=tau<21', 'tau>=21')))

# Use ggpairs() to construct a scatter plot matrix with different colors based on the three groups of cars:
ggpairs(data = SC %>% filter(!is.na(S100_log)), columns = 14:20,
        mapping = aes(colour = sports_type),
        upper = list(continous = wrap('cor', size = 3)))
```

### R Code Lecture Slides Part IV A

Define new features

```{r}
SC <- SC %>% mutate(x1 = log(weight/max_power),
                        x2 = log(max_power/cubic_capacity),
                        x3 = log(max_torque),
                        x4 = log(max_engine_speed),
                        x5 = log(cubic_capacity))
```

Use the ggpairs () function for constructing a scatter plot matrix with different colors based on the three groups of cars :

```{r}
ggpairs(SC , columns = 22:26 , mapping = aes(colour = sports_type ), upper = list( continuous = wrap ("cor", size = 5)))
```

Now paste the custom function for the empirical density plots:

```{r}
density_plots <- function(i) {
  # Step 1: Select the variables for which you will plot their empirical density plots
  # Convert the string to a symbol and then evaluate it inside select
  var <- SC %>% dplyr::select(paste0('x', i)) %>% pull
  
  # Step 2: Compute the empirical density estimates
  dens <- density(var)
  
  # Step 3: Create a data frame for the Gaussian approximation
  dat_norm <- data.frame(x = dens$x, y = dens$y, 
                         z = dnorm(dens$x, mean = mean(var), sd = sd(var)))
  
  # Step 4: The plot with the empirical density in red and the Gaussian approximation in blue
  p <- ggplot(data = dat_norm) + 
    geom_line(aes(x=x, y=y), col=1, lwd=1) + 
    geom_line(aes(x=x, y=z), col=4, lwd=1, linetype = "dashed") +
    labs(x = paste0("x", i), y = "density",
         title = paste0("Density of x", i))
  p
}
```

Let us see what they tell us. Call each of the 5 density plots below:

```{r}
density_plots(1)
density_plots(2)
density_plots(3)
density_plots(4)
density_plots(5)
```
Alternatively, we can check the QQ plots to see if the variables are normally distributed: `par(mfrow = c(a,b))` allows to split the screen into a grid of a rows and b columns.

```{r}
par(mfrow = c(1,5))
for(i in 1:5){
  var <-  SC %>% dplyr::select(paste0('x',i)) %>% pull
  qqnorm(var,main = paste0('x',i));qqline(var,col=2,lty=2,lwd=2)
}
```
#### Principal Component Analysis

Step 1: Use the new set of features:

```{r}
SC <- SC %>% mutate(x1 = log(weight/max_power),
                    x2 = log(max_power/cubic_capacity),
                    x3 = log(max_torque),
                    x4 = log(max_engine_speed),
                    x5 = log(cubic_capacity))
```

Step 2: Create the raw design matrix and name it `X_raw`, which includes `x1` to `x5`.

```{r}
X_raw <- SC %>% dplyr::select(x1,x2,x3,x4,x5)
```

Step 3: Standardize the raw design matrix `Xraw` by column means and column standard deviations: 

```{r}
X <- apply(X_raw, 2, function(x) (x - mean(x))/sd(x))
```

Now check the correlation matrix

```{r}
#The correlation matrix:
var(X)
#OR
cor(X)
```

We can also visualize it using the `ggcorplot` library

```{r}
library(ggcorrplot)
ggcorrplot(cor(X))
```

### R Code Lecture Slides Part IV B

Step 1: Run the codes above (redundant as we are in a notebook)

Step 2: Compute the SVD of the scaled matrix `X`

```{r}
svdX <- svd(X)
```

Step 3: Display the structure of the `svdX` object using the `str()` function:

```{r}
str(svdX)
```
Step 4: Extract the `V` matrix:

```{r}
V <- svdX$v
```

Step 5: Check the orthogonality of the matrices `V` and `t(V)`:

```{r}
t(V) %*% V
```

Note that the off-diagonal elements are not exactly 0 but they are very close to 0 which is fine.

#### PC Scratch

Now, we construct the principal components and summarize the results.

Step 1: Compute the principal components (PCs) as `Y=XV`. Each column of `Y` represents a `PC` and each column of `V` provides the coefficients for `x1,...,x5`.

```{r}
Y <- X %*% V
```

Step 2: Extract the singular values for each PC and compute the standard deviation of each principal component, the variation explained by each `PC` and the cumulative variation:

```{r}
pca_summary <- data.frame(
	p = 1:ncol(X),
	sng_values = svdX$d,
	sd_pcs = sqrt(svdX$d^2/(nrow(X)-1)),
	explained_variation = svdX$d^2/sum(svdX$d^2),
	cum_variation = cumsum(svdX$d^2/sum(svdX$d^2))
	)

```

Step 3: Summarize the results:

```{r}
pca_summary
```
#### `princomp`

Perform PCA using the `princomp()` function

Step 1: Use the `princomp()` function for the scaled matrix `X` and set `cor=TRUE` which means that the calculation will be based on eigen-decomposition of the matrix `A = t(X) %*% X`:

```{r}
pca <- princomp(X,cor=TRUE)
```

Step 2: Summarize the results:

```{r}
summary(pca)
```

Step 3: Compare with the previous results:

```{r}
pca_summary
```
Display the structure of the `pca` object using the `str()` function in R:

```{r}
str(pca)
```

Now we compare the eigenvectors which are computed using the `svd()` function to those which are calculated via the `princomp()` function. 

```{r}
#scd() vs pca() for the 1st eigenvectors:
V[,1]; pca$loadings[,1]   
```
#### Plots

Next, we plot a scatter plot to show the cars represented by the five principal components coordinates.

Step 1: Give names to the PCs and combine thim with sports car data set using `cbind()`:

```{r}
colnames(Y) <- names(c(PC = 1:5))
SC <- cbind(SC, Y)
```

Step 2: Plot the scatter plots using the `ggpairs()` function:

```{r}
ggpairs(data.frame(Y), upper = NULL)
```

Next, we graphically illustrate all cars along the first two principal component axis:

Step 1: Classify the cars using the values for `tau`:

```{r}
SC <- SC %>% mutate(sports_type = cut(tau,breaks = c(0, 17, 21, 100),
labels = c("tau<17 (sports car)","17<=tau<21", "tau>=21")))
```

Step 2: Plot the three types of cars using the first two principal components:

```{r}
ggplot(SC, aes(x = PC1, y = PC2, colour = sports_type)) + geom_point() +
labs(y = paste0("principal component ", 2),
x = paste0("principal component ", 1),
title = "Principal Component Analysis (PCA)")
```

We can also graphically illustrate the same results using the `biplot()` function in R that we also used in the previous class.

Step 1: Use `biplot()` function with the first two PCs in the first argument and the laodings (eigenvectors) in the second argument:

Step 2: Add labels to the graph for the first two PCs:

```{r}
biplot(Y[,1:2],V[,1:2], expand = 2,
xlab = "1st principal component", ylab = "2nd principal component",
cex = c(0.4, 1.5), ylim = c(-7, 7), xlim = c(-7, 7))

```
#### Reconstruction Errors

The reconstruction error can be seen as average squared
distance between the original data points and respective
projections onto principal subspace.

The reconstruction errors are scaled by $\sqrt(n)$. We use the `for()` loop to compute these and we observe that for `p = 2` PCs we receive a reconstruction error of $0.6124$.

```{r}
recon_err <- 0
for(p in 1:5){
Xp <- X %*% V[,1:p] %*% t(V[,1:p])
recon_err[p] <- sqrt(sum( (X - Xp)^2)/nrow(X))
			}
round(recon_err,5)

```

